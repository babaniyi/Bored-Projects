{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional programming\n",
      "\n",
      "In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can.\n",
      "Lambda calculus forms the basis of all functional programming languages.\n",
      "=== Pure functions ===\n",
      "\n",
      "Pure functions (or expressions) have no side effects (memory or I/O).\n",
      "Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations.\n",
      "Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\n",
      "Common patterns of recursion can be abstracted away using higher-order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples.\n",
      "Such recursion schemes play a role analogous to built-in control structures such as loops in imperative languages.\n",
      "=== Strict versus non-strict evaluation ===\n",
      "\n",
      "Functional languages can be categorized by whether they use strict (eager) or non-strict (lazy) evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated.\n",
      "Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\n",
      "While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well.\n",
      "For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap,  are based on arrays.\n",
      "Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified.\n",
      "For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game.\n",
      "However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles).\n",
      "Python had support for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2, though Python 3 relegated  \"reduce\" to the functools standard library module.\n",
      "== Applications ==\n",
      "\n",
      "\n",
      "=== Academia ===\n",
      "Functional programming is an active area of research in the field of programming language theory.\n",
      "There are several peer-reviewed publication venues focusing on functional programming, including the International Conference on Functional Programming, the Journal of Functional Programming, and the Symposium on Trends in Functional Programming.\n",
      "Haskell, though initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.Other functional programming languages that have seen use in industry include Scala, F#, Wolfram Language, Lisp, Standard ML, and Clojure.Functional \"platforms\" have been popular in finance for risk analytics (particularly with the larger investment banks).\n",
      "Some use it as their introduction to programming, while others teach it after teaching imperative programming.Outside of computer science, functional programming is being used as a method to teach problem solving, algebra and geometric concepts.\n",
      "Higher-Order Perl.\n",
      "****************************************************************************************************\n",
      "Automatic summarization\n",
      "\n",
      "Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "This problem is called multi-document summarization.\n",
      "Image collection summarization is another application example of automatic summarization.\n",
      "Video summarization is a related domain, where the system automatically creates a trailer of a long video.\n",
      "These algorithms model notions like diversity, coverage, information and representativeness of the summary.\n",
      "Keyphrases have many applications.\n",
      "Using the known keyphrases, we can assign positive or negative labels to the examples.\n",
      "The two measures can be combined in an F-score, which is the\n",
      "harmonic mean of the two (F = 2PR/(P + R) ).\n",
      "Thus, recall may suffer.\n",
      "In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.\n",
      "While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "Edges are created based on word co-occurrence in this application of TextRank.\n",
      "As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\n",
      "One way to think about it is the following.\n",
      "A word that appears multiple times throughout a text may have many different co-occurring neighbors.\n",
      "=== Document summarization ===\n",
      "Like keyphrase extraction, document summarization aims to identify the essence of a text.\n",
      "Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.\n",
      "First summarizes that perform adaptive summarization have been created.\n",
      "==== TextRank and LexRank ====\n",
      "The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.\n",
      "Then the sentences can be ranked with regard to their similarity to this centroid sentence.\n",
      "The task remains the same in both casesâ€”only the number of sentences to choose from has grown.\n",
      "Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary.\n",
      "The methods are domain-independent and easily portable.\n",
      "The set cover function attempts to find a subset of objects which cover a given set of concepts.\n",
      "All these important models encouraging coverage, diversity and information are all submodular.\n",
      "Similarly, work by Lin and Bilmes, 2011, shows that many existing systems for automatic summarization are instances of submodular functions.\n",
      "Submodular Functions have also successfully been used for summarizing machine learning datasets.\n",
      "== Evaluation techniques ==\n",
      "The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\n",
      "For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.\n",
      "The Use of Topic Segmentation for Automatic Summarization.\n"
     ]
    }
   ],
   "source": [
    "# Functional Progamming: Text Summarization Using SumBasic Method and NLKT\n",
    "\n",
    "import wikipedia\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "def data(*args):\n",
    "    contents = []\n",
    "    for arg in args:\n",
    "        contents.append((arg, wikipedia.page(arg).content))\n",
    "    return contents\n",
    "\n",
    "def sentence_tokenizer(data):\n",
    "    header, datum = data\n",
    "    print(header)\n",
    "    print()\n",
    "    return sent_tokenize(datum)\n",
    "\n",
    "def word_tokenizer(sentence_tokenizer):\n",
    "    \n",
    "    word_tokens = []\n",
    "    sentence_tokens = {sentence: [] for sentence in sentence_tokenizer}\n",
    "    \n",
    "    for one_sentence in sentence_tokenizer:\n",
    "        for token in regexp_tokenize(one_sentence.lower(), '\\w+'):\n",
    "            if token not in string.punctuation:\n",
    "                if token not in stopwords.words('english'):\n",
    "                    word_tokens.append(token)\n",
    "                    sentence_tokens[one_sentence].append(token)\n",
    "    return sentence_tokens, word_tokens\n",
    "    \n",
    "\n",
    "def lemmatizer(word_tokenizer):\n",
    "    sentence_tokens, word_tokens = word_tokenizer\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    lem_words = [lem.lemmatize(word) for word in word_tokens]\n",
    "    lem_sentences = {sentence: [lem.lemmatize(word) for word in sentence_tokens[sentence]] for sentence in sentence_tokens}\n",
    "    return lem_sentences, lem_words\n",
    "\n",
    "def sumbasic(lemmatizer):\n",
    "    lem_sentences, lem_words = lemmatizer\n",
    "    \n",
    "    freq = FreqDist(lem_words)\n",
    "    total = sum(freq.values())\n",
    "    probs = {k: v/total for k, v in freq.items()}\n",
    "    \n",
    "    len_summary = int(0.1 * len(lem_sentences))\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    for _ in range(len_summary):\n",
    "        \n",
    "        scores = {k: [] for k in lem_sentences}\n",
    "        importance = {k: 0 for k in scores}\n",
    "        for key, value in lem_sentences.items():\n",
    "            for word in value:\n",
    "                scores[key].append(probs[word])\n",
    "            importance[key] = sum(scores[key]) / len(scores[key])         \n",
    "            \n",
    "        most_importance_sentence = max(scores, key=scores.get)\n",
    "        summary.append(most_importance_sentence)\n",
    "        \n",
    "        for word in lem_sentences[most_importance_sentence]:\n",
    "            probs[word] = probs[word] * probs[word]\n",
    "            \n",
    "    for sentence in lem_sentences:\n",
    "        if sentence in summary:\n",
    "            print(sentence)\n",
    "\n",
    "for i, datum in enumerate(data('Functional programming', 'Automatic summarization')):\n",
    "    sumbasic(lemmatizer(word_tokenizer(sentence_tokenizer(datum))))\n",
    "    if i == 0:\n",
    "        print('*'*100, sep='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
